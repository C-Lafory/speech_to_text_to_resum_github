import os
import logging
import subprocess
import shutil

# Configuration
MODEL_DIR = "models"
WHISPER_MODEL_SIZE = "medium"
SPACY_MODEL = "fr_core_news_sm"
TTS_MODEL = "tts_models/multilingual/multi-dataset/your_tts"
MIN_DISK_SPACE_GB = 10  # Espace disque minimum requis en GB

# D√©terminer quel service ex√©cute le script
IS_TTS_SERVICE = os.path.exists("text_to_speech.py")
IS_MAIN_SERVICE = os.path.exists("transcription.py")

# Importer les modules en fonction du service
if IS_MAIN_SERVICE:
    import whisper
    import spacy
if IS_TTS_SERVICE:
    from TTS.api import TTS

def check_disk_space():
    """V√©rifie l'espace disque disponible"""
    total, used, free = shutil.disk_usage("/")
    free_gb = free // (2**30)  # Conversion en GB
    if free_gb < MIN_DISK_SPACE_GB:
        raise RuntimeError(f"‚ùå Espace disque insuffisant. {free_gb}GB disponible, {MIN_DISK_SPACE_GB}GB requis.")

def verify_models():
    """V√©rifie si tous les mod√®les sont pr√©sents"""
    models_status = {
        "whisper": False,
        "spacy": False,
        "tts": False,
        "ollama": False,
        "mistral": False
    }
    
    # V√©rification Whisper (uniquement pour le service principal)
    if IS_MAIN_SERVICE:
        whisper_model_path = os.path.join(MODEL_DIR, WHISPER_MODEL_SIZE + ".pt")
        models_status["whisper"] = os.path.isfile(whisper_model_path)
        
        # V√©rification Spacy
        try:
            spacy.load(SPACY_MODEL)
            models_status["spacy"] = True
        except OSError:
            pass
    
    # V√©rification TTS (uniquement pour le service TTS)
    if IS_TTS_SERVICE:
        try:
            TTS(model_name=TTS_MODEL, progress_bar=False)
            models_status["tts"] = True
        except Exception:
            pass
    
    # V√©rification Ollama (uniquement pour le service principal)
    if IS_MAIN_SERVICE:
        try:
            subprocess.run(["ollama", "--version"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            models_status["ollama"] = True
        except (FileNotFoundError, subprocess.CalledProcessError):
            pass
        
        # V√©rification Mistral
        if models_status["ollama"]:
            try:
                subprocess.run(["ollama", "list"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
                models_status["mistral"] = "mistral:7b" in subprocess.run(["ollama", "list"], capture_output=True, text=True).stdout
            except subprocess.CalledProcessError:
                pass
    
    return models_status

def download_models():
    """T√©l√©charge tous les mod√®les n√©cessaires et cr√©e le dossier si n√©cessaire."""
    # V√©rification de l'espace disque
    check_disk_space()
    
    # Cr√©er le dossier des mod√®les s'il n'existe pas
    if not os.path.exists(MODEL_DIR):
        logging.info(f"üìÅ Cr√©ation du dossier des mod√®les : {MODEL_DIR}")
        os.makedirs(MODEL_DIR, exist_ok=True)

    # 1. V√©rification et t√©l√©chargement du mod√®le Whisper (uniquement pour le service principal)
    if IS_MAIN_SERVICE:
        logging.info(f"üîç V√©rification du mod√®le Whisper ({WHISPER_MODEL_SIZE})...")
        whisper_model_path = os.path.join(MODEL_DIR, WHISPER_MODEL_SIZE + ".pt")
        if not os.path.isfile(whisper_model_path):
            logging.info(f"‚¨áÔ∏è T√©l√©chargement du mod√®le Whisper ({WHISPER_MODEL_SIZE})...")
            whisper.load_model(WHISPER_MODEL_SIZE, download_root=MODEL_DIR)
            logging.info(f"‚úÖ Mod√®le Whisper t√©l√©charg√© dans {MODEL_DIR}")
        else:
            logging.info(f"‚úÖ Mod√®le Whisper d√©j√† pr√©sent : {whisper_model_path}")

        # 2. V√©rification et t√©l√©chargement du mod√®le Spacy
        logging.info(f"üîç V√©rification du mod√®le Spacy ({SPACY_MODEL})...")
        try:
            spacy.load(SPACY_MODEL)
            logging.info(f"‚úÖ Mod√®le Spacy d√©j√† pr√©sent")
        except OSError:
            logging.info(f"‚¨áÔ∏è T√©l√©chargement du mod√®le Spacy ({SPACY_MODEL})...")
            spacy.cli.download(SPACY_MODEL)
            logging.info(f"‚úÖ Mod√®le Spacy t√©l√©charg√©")

    # 3. V√©rification et t√©l√©chargement du mod√®le TTS (uniquement pour le service TTS)
    if IS_TTS_SERVICE:
        logging.info(f"üîç V√©rification du mod√®le TTS ({TTS_MODEL})...")
        try:
            tts = TTS(model_name=TTS_MODEL, progress_bar=False)
            if tts.speakers and len(tts.speakers) > 0:
                logging.info(f"‚úÖ Mod√®le TTS d√©j√† pr√©sent et fonctionnel")
            else:
                raise RuntimeError("Mod√®le TTS pr√©sent mais pas de locuteurs disponibles")
        except Exception as e:
            logging.info(f"‚¨áÔ∏è T√©l√©chargement du mod√®le TTS ({TTS_MODEL})...")
            tts = TTS(model_name=TTS_MODEL, progress_bar=True)
            if not tts.speakers or len(tts.speakers) == 0:
                raise RuntimeError("Mod√®le TTS t√©l√©charg√© mais pas de locuteurs disponibles")
            logging.info(f"‚úÖ Mod√®le TTS t√©l√©charg√©")

    # 4. V√©rification et installation du client Ollama (uniquement pour le service principal)
    if IS_MAIN_SERVICE:
        logging.info("üîç V√©rification du client Ollama...")
        try:
            subprocess.run(["ollama", "--version"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            logging.info("‚úÖ Client Ollama d√©j√† install√©.")
        except (FileNotFoundError, subprocess.CalledProcessError):
            logging.info("‚¨áÔ∏è Installation du client Ollama...")
            try:
                # V√©rifier si curl est install√©
                subprocess.run(["curl", "--version"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            except (FileNotFoundError, subprocess.CalledProcessError):
                logging.info("‚¨áÔ∏è Installation de curl...")
                subprocess.run(["apt-get", "update"], check=True)
                subprocess.run(["apt-get", "install", "-y", "curl"], check=True)
            
            try:
                install_command = "curl -fsSL https://ollama.com/install.sh | sh"
                result = subprocess.run(install_command, shell=True, capture_output=True, text=True)
                if result.returncode != 0:
                    raise RuntimeError(f"√âchec de l'installation d'Ollama: {result.stderr}")
                logging.info("‚úÖ Client Ollama install√© avec succ√®s.")
            except Exception as e:
                logging.error(f"‚ùå √âchec de l'installation du client Ollama : {e}")
                logging.warning("‚ö†Ô∏è Ollama n'est pas install√©. Le r√©sum√© ne sera pas disponible.")
                return  # On continue sans Ollama

        # 5. V√©rification du mod√®le Mistral via Ollama
        logging.info("üîç V√©rification du mod√®le Mistral via Ollama...")
        try:
            subprocess.run(["ollama", "pull", "mistral:7b"], check=True)
            logging.info("‚úÖ Mod√®le Mistral v√©rifi√©/install√© via Ollama")
        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la v√©rification du mod√®le Mistral : {e}")
            logging.warning("‚ö†Ô∏è Le mod√®le Mistral n'est pas disponible. Le r√©sum√© ne sera pas disponible.")
            return  # On continue sans Mistral

    # V√©rification finale
    models_status = verify_models()
    if all(models_status.values()):
        logging.info("‚úÖ Tous les mod√®les sont correctement install√©s et fonctionnels")
    else:
        missing_models = [model for model, status in models_status.items() if not status]
        logging.warning(f"‚ö†Ô∏è Certains mod√®les ne sont pas correctement install√©s : {', '.join(missing_models)}")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    download_models()
    