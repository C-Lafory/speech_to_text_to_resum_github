import os
import sys
import shutil
import logging
import requests
from pathlib import Path
import torch
from config import IS_MAIN_SERVICE, IS_TTS_SERVICE, OLLAMA_MODEL

# Configuration des logs
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# Configuration de l'espace disque
MIN_DISK_SPACE_GB = 5  # Espace disque minimum requis en GB

# Chemins des mod√®les
MODELS_DIR = Path("models")
WHISPER_MODEL_SIZE = "base"
TTS_MODEL = "tts_models/fr/css10/vits"  # Alias pour TTS_MODEL_NAME
TTS_MODEL_NAME = "tts_models/fr/css10/vits"
SPACY_MODEL_NAME = "fr_core_news_md"

# Configuration Ollama
OLLAMA_API_URL = "http://0.0.0.0:11434/api"

# Cr√©ation des r√©pertoires
MODELS_DIR.mkdir(exist_ok=True)
WHISPER_DIR = MODELS_DIR / "whisper"
TTS_DIR = MODELS_DIR / "tts"
SPACY_DIR = MODELS_DIR / "spacy"
OLLAMA_DIR = MODELS_DIR / "ollama"

# Import conditionnel des modules
if IS_MAIN_SERVICE:
    import whisper
    import spacy

if IS_TTS_SERVICE:
    from TTS.utils.manage import ModelManager
    from TTS.utils.synthesizer import Synthesizer
    from TTS.api import TTS

def check_disk_space():
    """V√©rifie l'espace disque disponible"""
    total, used, free = shutil.disk_usage("/")
    free_gb = free // (2**30)  # Conversion en GB
    if free_gb < MIN_DISK_SPACE_GB:
        raise RuntimeError(f"‚ùå Espace disque insuffisant. {free_gb}GB disponible, {MIN_DISK_SPACE_GB}GB requis.")

def check_ollama_status():
    """V√©rifie le statut d'Ollama via l'API"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/version")
        if response.status_code == 200:
            return True
    except requests.exceptions.RequestException:
        return False
    return False

def check_mistral_model():
    """V√©rifie si le mod√®le Mistral est disponible"""
    try:
        response = requests.get(f"{OLLAMA_API_URL}/tags")
        if response.status_code == 200:
            models = response.json().get("models", [])
            return any(model["name"] == OLLAMA_MODEL for model in models)
    except requests.exceptions.RequestException:
        return False
    return False

def verify_models():
    """V√©rifie si tous les mod√®les sont pr√©sents"""
    models_status = {
        "whisper": False,
        "spacy": False,
        "tts": False,
        "ollama": False,
        "mistral": False
    }
    
    # V√©rification Whisper (uniquement pour le service principal)
    if IS_MAIN_SERVICE:
        whisper_model_path = WHISPER_DIR / f"{WHISPER_MODEL_SIZE}.pt"
        models_status["whisper"] = whisper_model_path.exists()
        
        # V√©rification Spacy
        try:
            spacy.load(SPACY_MODEL_NAME)
            models_status["spacy"] = True
        except (OSError, ImportError):
            pass
        
        # V√©rification Ollama et Mistral
        models_status["ollama"] = check_ollama_status()
        if models_status["ollama"]:
            models_status["mistral"] = check_mistral_model()
    
    # V√©rification TTS (uniquement pour le service TTS)
    if IS_TTS_SERVICE:
        try:
            tts = TTS(model_name=TTS_MODEL_NAME, progress_bar=False)
            models_status["tts"] = True
        except Exception:
            pass
    
    return models_status

def download_models():
    """T√©l√©charge tous les mod√®les n√©cessaires"""
    # V√©rification de l'espace disque
    check_disk_space()

    # 1. V√©rification et t√©l√©chargement du mod√®le Whisper
    if IS_MAIN_SERVICE:
        logging.info(f"üîç V√©rification du mod√®le Whisper ({WHISPER_MODEL_SIZE})...")
        whisper_model_path = WHISPER_DIR / f"{WHISPER_MODEL_SIZE}.pt"
        if not whisper_model_path.exists():
            logging.info(f"‚¨áÔ∏è T√©l√©chargement du mod√®le Whisper ({WHISPER_MODEL_SIZE})...")
            whisper.load_model(WHISPER_MODEL_SIZE, download_root=str(WHISPER_DIR))
            logging.info(f"‚úÖ Mod√®le Whisper t√©l√©charg√© dans {WHISPER_DIR}")
        else:
            logging.info(f"‚úÖ Mod√®le Whisper d√©j√† pr√©sent : {whisper_model_path}")

        # 2. V√©rification et t√©l√©chargement du mod√®le Spacy
        logging.info(f"üîç V√©rification du mod√®le Spacy ({SPACY_MODEL_NAME})...")
        try:
            spacy.load(SPACY_MODEL_NAME)
            logging.info(f"‚úÖ Mod√®le Spacy d√©j√† pr√©sent")
        except (OSError, ImportError):
            logging.info(f"‚¨áÔ∏è T√©l√©chargement du mod√®le Spacy ({SPACY_MODEL_NAME})...")
            spacy.cli.download(SPACY_MODEL_NAME)
            logging.info(f"‚úÖ Mod√®le Spacy t√©l√©charg√©")

    # 3. V√©rification et t√©l√©chargement du mod√®le TTS
    if IS_TTS_SERVICE:
        logging.info(f"üîç V√©rification du mod√®le TTS ({TTS_MODEL_NAME})...")
        try:
            tts = TTS(model_name=TTS_MODEL_NAME, progress_bar=False)
            if tts.speakers and len(tts.speakers) > 0:
                logging.info(f"‚úÖ Mod√®le TTS d√©j√† pr√©sent et fonctionnel")
            else:
                raise RuntimeError("Mod√®le TTS pr√©sent mais pas de locuteurs disponibles")
        except Exception as e:
            logging.info(f"‚¨áÔ∏è T√©l√©chargement du mod√®le TTS ({TTS_MODEL_NAME})...")
            tts = TTS(model_name=TTS_MODEL_NAME, progress_bar=True)
            if not tts.speakers or len(tts.speakers) == 0:
                raise RuntimeError("Mod√®le TTS t√©l√©charg√© mais pas de locuteurs disponibles")
            logging.info(f"‚úÖ Mod√®le TTS t√©l√©charg√©")

    # V√©rification finale
    models_status = verify_models()
    if all(models_status.values()):
        logging.info("‚úÖ Tous les mod√®les sont correctement install√©s et fonctionnels")
    else:
        missing_models = [model for model, status in models_status.items() if not status]
        logging.warning(f"‚ö†Ô∏è Certains mod√®les ne sont pas correctement install√©s : {', '.join(missing_models)}")

if __name__ == "__main__":
    download_models()
    